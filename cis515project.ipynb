{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b52bce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kamayanirai/anaconda3/envs/hfenv/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6c1b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: 0.26.0 not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install \"transformers[torch]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4559bbab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kamayanirai/anaconda3/envs/hfenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kamayanirai/anaconda3/envs/hfenv/lib/python3.10/site-packages/transformers/__init__.py\n",
      "4.51.3\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__file__)\n",
    "print(transformers.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b09f1533",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kamayanirai/anaconda3/envs/hfenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6264b7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Imports ---\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import torch\n",
    "\n",
    "# --- Step 2: Load ingredients from JSON ---\n",
    "with open('ingredient.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "df = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a06ddfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers.training_args\n",
      "['_VALID_DICT_FIELDS', '__annotations__', '__class__', '__dataclass_fields__', '__dataclass_params__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__match_args__', '__module__', '__ne__', '__new__', '__post_init__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_dict_torch_dtype_to_str', '_n_gpu', '_no_sync_in_gradient_accumulation', '_setup_devices', 'accelerator_config', 'adafactor', 'adam_beta1', 'adam_beta2', 'adam_epsilon', 'auto_find_batch_size', 'average_tokens_across_devices', 'batch_eval_metrics', 'bf16', 'bf16_full_eval', 'data_seed', 'dataloader_drop_last', 'dataloader_num_workers', 'dataloader_persistent_workers', 'dataloader_pin_memory', 'dataloader_prefetch_factor', 'ddp_backend', 'ddp_broadcast_buffers', 'ddp_bucket_cap_mb', 'ddp_find_unused_parameters', 'ddp_timeout', 'ddp_timeout_delta', 'debug', 'deepspeed', 'default_optim', 'device', 'disable_tqdm', 'do_eval', 'do_predict', 'do_train', 'eval_accumulation_steps', 'eval_batch_size', 'eval_delay', 'eval_do_concat_batches', 'eval_on_start', 'eval_steps', 'eval_strategy', 'eval_use_gather_object', 'fp16', 'fp16_backend', 'fp16_full_eval', 'fp16_opt_level', 'framework', 'fsdp', 'fsdp_config', 'fsdp_min_num_params', 'fsdp_transformer_layer_cls_to_wrap', 'full_determinism', 'get_process_log_level', 'get_warmup_steps', 'gradient_accumulation_steps', 'gradient_checkpointing', 'gradient_checkpointing_kwargs', 'greater_is_better', 'group_by_length', 'half_precision_backend', 'hub_always_push', 'hub_model_id', 'hub_private_repo', 'hub_strategy', 'hub_token', 'ignore_data_skip', 'include_inputs_for_metrics', 'include_num_input_tokens_seen', 'include_tokens_per_second', 'jit_mode_eval', 'label_names', 'label_smoothing_factor', 'learning_rate', 'length_column_name', 'load_best_model_at_end', 'local_process_index', 'local_rank', 'log_level', 'log_level_replica', 'log_on_each_node', 'logging_dir', 'logging_first_step', 'logging_nan_inf_filter', 'logging_steps', 'logging_strategy', 'lr_scheduler_type', 'main_process_first', 'max_grad_norm', 'max_steps', 'metric_for_best_model', 'mp_parameters', 'n_gpu', 'neftune_noise_alpha', 'no_cuda', 'num_train_epochs', 'optim', 'optim_args', 'optim_target_modules', 'output_dir', 'overwrite_output_dir', 'parallel_mode', 'past_index', 'per_device_eval_batch_size', 'per_device_train_batch_size', 'per_gpu_eval_batch_size', 'per_gpu_train_batch_size', 'place_model_on_device', 'prediction_loss_only', 'process_index', 'push_to_hub', 'push_to_hub_model_id', 'push_to_hub_organization', 'push_to_hub_token', 'ray_scope', 'remove_unused_columns', 'report_to', 'restore_callback_states_from_checkpoint', 'resume_from_checkpoint', 'run_name', 'save_on_each_node', 'save_only_model', 'save_safetensors', 'save_steps', 'save_strategy', 'save_total_limit', 'seed', 'set_dataloader', 'set_evaluate', 'set_logging', 'set_lr_scheduler', 'set_optimizer', 'set_push_to_hub', 'set_save', 'set_testing', 'set_training', 'should_log', 'should_save', 'skip_memory_metrics', 'tf32', 'to_dict', 'to_json_string', 'to_sanitized_dict', 'torch_compile', 'torch_compile_backend', 'torch_compile_mode', 'torch_empty_cache_steps', 'torchdynamo', 'tp_size', 'tpu_metrics_debug', 'tpu_num_cores', 'train_batch_size', 'use_cpu', 'use_ipex', 'use_legacy_prediction_loop', 'use_liger_kernel', 'use_mps_device', 'warmup_ratio', 'warmup_steps', 'weight_decay', 'world_size']\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "print(TrainingArguments.__module__)  # This will show where the class is coming from\n",
    "print(dir(TrainingArguments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "714e81c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 293/293 [00:00<00:00, 10208.68 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# --- Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import json\n",
    "\n",
    "# --- Step 1: Load the data ---\n",
    "# Load your ingredient.json file\n",
    "with open('ingredient.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# --- Step 2: Encode Labels ---\n",
    "# Create mapping dictionaries\n",
    "unique_labels = df[\"label\"].unique().tolist()\n",
    "label2id = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "# Map labels to integers\n",
    "df[\"labels\"] = df[\"label\"].map(label2id)\n",
    "\n",
    "df = df.drop(columns=[\"label\"])    # ðŸ›‘ IMPORTANT!!\n",
    "\n",
    "# --- Step 3: Load into Huggingface Dataset ---\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "\n",
    "# --- Step 4: Tokenizer and Model ---\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=128)\n",
    "\n",
    "# Important: Correct Tokenize function\n",
    "def tokenize_function(examples):\n",
    "    tokenized = tokenizer(examples[\"ingredient\"], padding= True, truncation=True)\n",
    "    tokenized[\"labels\"] = examples[\"labels\"]  # Already mapped to int\n",
    "    return tokenized\n",
    "\n",
    "# Apply tokenizer\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae1ca18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# --- Step 5: Train/Test Split ---\n",
    "train_test_split = tokenized_dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "eval_dataset = train_test_split[\"test\"]\n",
    "\n",
    "# --- Step 6: Load Model ---\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(unique_labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "# --- Step 7: Metrics ---\n",
    "def compute_metrics(pred):\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    labels = pred.label_ids\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0b45089",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pt/dcyg6j9x4bj4f_2gx863y7mw0000gn/T/ipykernel_15674/2349746199.py:20: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/Users/kamayanirai/anaconda3/envs/hfenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [300/300 01:50, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>4.386034</td>\n",
       "      <td>0.576271</td>\n",
       "      <td>0.550290</td>\n",
       "      <td>0.603909</td>\n",
       "      <td>0.576271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>4.740614</td>\n",
       "      <td>0.559322</td>\n",
       "      <td>0.516564</td>\n",
       "      <td>0.648207</td>\n",
       "      <td>0.559322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>3.751189</td>\n",
       "      <td>0.677966</td>\n",
       "      <td>0.660065</td>\n",
       "      <td>0.689483</td>\n",
       "      <td>0.677966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>3.775083</td>\n",
       "      <td>0.661017</td>\n",
       "      <td>0.648201</td>\n",
       "      <td>0.681714</td>\n",
       "      <td>0.661017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>3.813960</td>\n",
       "      <td>0.661017</td>\n",
       "      <td>0.648201</td>\n",
       "      <td>0.681714</td>\n",
       "      <td>0.661017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>3.841349</td>\n",
       "      <td>0.677966</td>\n",
       "      <td>0.660065</td>\n",
       "      <td>0.689483</td>\n",
       "      <td>0.677966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>3.862129</td>\n",
       "      <td>0.677966</td>\n",
       "      <td>0.660065</td>\n",
       "      <td>0.689483</td>\n",
       "      <td>0.677966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>3.880484</td>\n",
       "      <td>0.661017</td>\n",
       "      <td>0.648201</td>\n",
       "      <td>0.681714</td>\n",
       "      <td>0.661017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.889596</td>\n",
       "      <td>0.661017</td>\n",
       "      <td>0.648201</td>\n",
       "      <td>0.681714</td>\n",
       "      <td>0.661017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.892230</td>\n",
       "      <td>0.661017</td>\n",
       "      <td>0.648201</td>\n",
       "      <td>0.681714</td>\n",
       "      <td>0.661017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kamayanirai/anaconda3/envs/hfenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/kamayanirai/anaconda3/envs/hfenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/kamayanirai/anaconda3/envs/hfenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/kamayanirai/anaconda3/envs/hfenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/kamayanirai/anaconda3/envs/hfenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/kamayanirai/anaconda3/envs/hfenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/kamayanirai/anaconda3/envs/hfenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/kamayanirai/anaconda3/envs/hfenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/kamayanirai/anaconda3/envs/hfenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/kamayanirai/anaconda3/envs/hfenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/kamayanirai/anaconda3/envs/hfenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/kamayanirai/anaconda3/envs/hfenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/kamayanirai/anaconda3/envs/hfenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/kamayanirai/anaconda3/envs/hfenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/kamayanirai/anaconda3/envs/hfenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/kamayanirai/anaconda3/envs/hfenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/kamayanirai/anaconda3/envs/hfenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/kamayanirai/anaconda3/envs/hfenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/kamayanirai/anaconda3/envs/hfenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/kamayanirai/anaconda3/envs/hfenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/kamayanirai/anaconda3/envs/hfenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=300, training_loss=0.004265719833298742, metrics={'train_runtime': 111.3997, 'train_samples_per_second': 21.005, 'train_steps_per_second': 2.693, 'total_flos': 18038791789200.0, 'train_loss': 0.004265719833298742, 'epoch': 10.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# --- Step 8: Trainer Arguments ---\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"ingredient_model\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate= 5e-5 ,\n",
    "    per_device_train_batch_size= 8,\n",
    "    per_device_eval_batch_size= 8,\n",
    "    num_train_epochs= 10 ,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True,\n",
    ")\n",
    "\n",
    "# --- Step 9: Trainer ---\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# --- Step 10: Train the model ---\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d84605c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ingredient_classifier_final/tokenizer_config.json',\n",
       " 'ingredient_classifier_final/special_tokens_map.json',\n",
       " 'ingredient_classifier_final/vocab.txt',\n",
       " 'ingredient_classifier_final/added_tokens.json',\n",
       " 'ingredient_classifier_final/tokenizer.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# --- Step 11: Save the fine-tuned model ---\n",
    "trainer.save_model(\"ingredient_classifier_final\")\n",
    "tokenizer.save_pretrained(\"ingredient_classifier_final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b1eb68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kamayanirai/Python/.conda/lib/python3.10/site-packages/transformers/__init__.py\n",
      "4.51.3\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__file__)\n",
    "print(transformers.__version__)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hfenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
